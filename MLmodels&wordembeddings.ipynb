{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "embedder = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Language Detection.csv')\n",
    "x = dataset['Text'].to_numpy()\n",
    "y = dataset['Language'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [embedder(converted_string) for converted_string in x]\n",
    "x = [v.vector for v in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def make_classifier(reviews , labels):\n",
    "    clf = RandomForestClassifier(n_estimators=100 , random_state=0 )\n",
    "    clf.fit(reviews , labels)\n",
    "    return clf\n",
    "classifier_ = make_classifier(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9383552176917761\n",
      "0.8497743391360413\n"
     ]
    }
   ],
   "source": [
    "print(classifier_.score(x_train , y_train))\n",
    "print(classifier_.score(x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9383552176917761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier_2 = GradientBoostingClassifier(n_estimators=100 , random_state=0)\n",
    "classifier_2.fit(x_train , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8455834945196647\n"
     ]
    }
   ],
   "source": [
    "print(classifier_2.score(x_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [embedder(converted_string) for converted_string in ['ich bin aus ayland']]\n",
    "x = [v.vector for v in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00019575892059226898,\n",
       " 6.536186697270984e-05,\n",
       " 3.930936781640891e-05,\n",
       " 3.061978382699012e-06,\n",
       " 4.158852407443805e-06,\n",
       " 0.9994174404434346,\n",
       " 6.26690259735485e-06,\n",
       " 2.013547670235933e-09,\n",
       " 9.641588784006352e-06,\n",
       " 1.1325144844053207e-05,\n",
       " 4.210104684632446e-05,\n",
       " 3.224614173073272e-05,\n",
       " 3.17418232069675e-06,\n",
       " 3.4535746343522256e-05,\n",
       " 7.783458593003339e-05,\n",
       " 3.1748681779357106e-05,\n",
       " 2.6032535670193618e-05]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_2.predict_proba(x).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arabic', 'Danish', 'Dutch', 'English', 'French', 'German',\n",
       "       'Greek', 'Hindi', 'Italian', 'Kannada', 'Malayalam', 'Portugeese',\n",
       "       'Russian', 'Spanish', 'Sweedish', 'Tamil', 'Turkish'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0.00019575892059226898,\n",
       " 'Danish': 6.536186697270984e-05,\n",
       " 'Dutch': 3.930936781640891e-05,\n",
       " 'English': 3.061978382699012e-06,\n",
       " 'French': 4.158852407443805e-06,\n",
       " 'German': 0.9994174404434346,\n",
       " 'Greek': 6.26690259735485e-06,\n",
       " 'Hindi': 2.013547670235933e-09,\n",
       " 'Italian': 9.641588784006352e-06,\n",
       " 'Kannada': 1.1325144844053207e-05,\n",
       " 'Malayalam': 4.210104684632446e-05,\n",
       " 'Portugeese': 3.224614173073272e-05,\n",
       " 'Russian': 3.17418232069675e-06,\n",
       " 'Spanish': 3.4535746343522256e-05,\n",
       " 'Sweedish': 7.783458593003339e-05,\n",
       " 'Tamil': 3.1748681779357106e-05,\n",
       " 'Turkish': 2.6032535670193618e-05}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classifier_2.classes_ , classifier_2.predict_proba(x).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_2.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model to pickle file\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "joblib.dump(classifier_ , 'classifier.pkl')\n",
    "joblib.dump(classifier_2 , 'classifier_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('classifier_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(text):\n",
    "    x = [embedder(converted_string) for converted_string in [text]]\n",
    "    x = [v.vector for v in x]\n",
    "    return dict(zip(classifier_2.classes_ , classifier_2.predict_proba(x).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoz\\anaconda3\\lib\\site-packages\\gradio\\inputs.py:26: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\antoz\\anaconda3\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\antoz\\anaconda3\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\antoz\\anaconda3\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\antoz\\anaconda3\\lib\\site-packages\\gradio\\outputs.py:196: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "Running on public URL: https://25845.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://25845.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x1d228204a00>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://25845.gradio.app')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "text_input = gr.inputs.Textbox(lines=3)\n",
    "label = gr.outputs.Label()\n",
    "examples = ['اهلا و سهلا , مرحبا' , \n",
    "            'hello , how are you my name is omar , iam from egypt and i really like gaming' ,\n",
    "            'Доброе утро , Здравствуйте , Меня зовут Мандли' , \n",
    "            'Bonjour , Comment allez-vous , S’il vous plaît , Je m’appelle , Je ne comprends pas']\n",
    "intf = gr.Interface(fn = pred , inputs = text_input , outputs = label , examples=examples)\n",
    "intf.launch(share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "73/73 [==============================] - 2s 10ms/step - loss: 0.9861 - accuracy: 0.7028 - val_loss: 1.5160 - val_accuracy: 0.6812\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.8415 - val_loss: 0.7268 - val_accuracy: 0.8095\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.3886 - accuracy: 0.8668 - val_loss: 0.4915 - val_accuracy: 0.8404\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.3550 - accuracy: 0.8745 - val_loss: 0.4282 - val_accuracy: 0.8617\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8809 - val_loss: 0.4412 - val_accuracy: 0.8504\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.3153 - accuracy: 0.8846 - val_loss: 0.4268 - val_accuracy: 0.8623\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.3015 - accuracy: 0.8898 - val_loss: 0.4459 - val_accuracy: 0.8540\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2813 - accuracy: 0.8983 - val_loss: 0.4116 - val_accuracy: 0.8607\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2820 - accuracy: 0.8969 - val_loss: 0.4039 - val_accuracy: 0.8646\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2582 - accuracy: 0.9038 - val_loss: 0.4296 - val_accuracy: 0.8627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d22a94df40>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a neural network with 2 layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense ,BatchNormalization\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_shape=(300,) )   ) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=17, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), y_train_one_hot, epochs=10, batch_size=100 ,validation_data=(np.array(x_test) , y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode y_train\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train_one_hot = lb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2b1884aac98f9b7855e8895f74d488ad518f49fce9c97dd1a17c527c00def65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
